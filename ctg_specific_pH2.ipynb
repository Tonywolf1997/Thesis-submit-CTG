{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ctg_specific_pH2.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1FGkeQWD-Bx0LvtT1fZ8lca5UvUGpR0GQ","authorship_tag":"ABX9TyPcZ/kfVCA3Bw3MmqPJ6/pG"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXY6_I8zS0Q0","executionInfo":{"status":"ok","timestamp":1632107120397,"user_tz":-420,"elapsed":847,"user":{"displayName":"Tony Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00046150741472937393"}},"outputId":"a7f1049d-acc4-4e58-9aca-c1f7a9fda413"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","import os\n","print(os.getcwd())\n","os.chdir('/content/drive/MyDrive/Colab_Notebooks/CTG_Workspace')\n","!pwd"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content\n","/content/drive/MyDrive/Colab_Notebooks/CTG_Workspace\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VkvEwwLLTFCt","executionInfo":{"status":"ok","timestamp":1632107126098,"user_tz":-420,"elapsed":5089,"user":{"displayName":"Tony Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00046150741472937393"}},"outputId":"a5ba6d01-164a-4236-9ee0-01f9addf4bec"},"source":["import pandas as pd\n","import numpy as np\n","import glob\n","import os\n","os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # Run on CPU\n","\n","# Make numpy values easier to read.\n","np.set_printoptions(precision=3, suppress=True)\n","\n","import tensorflow as tf\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers.experimental import preprocessing\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import MinMaxScaler\n","\n","\n","import ltc2_model as ltc\n","from ctrnn2_model import CTRNN, NODE, CTGRU\n","import argparse\n","import datetime as dt\n"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:101: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n"]}]},{"cell_type":"code","metadata":{"id":"U3MfNB-eTJ0A","executionInfo":{"status":"ok","timestamp":1632107126099,"user_tz":-420,"elapsed":5,"user":{"displayName":"Tony Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00046150741472937393"}}},"source":["class CtgData:  \n","    def data_frame(self):\n","        #Anotation Set\n","        ann_name = os.path.join(\"database\",\"ann_db_read.csv\")\n","        #print(ann_name)\n","        ann = pd.read_csv(ann_name,header=0, index_col=0)\n","        # \"database/ann_db_read.csv\", header=0, index_col=0\n","\n","        #CTG Sets\n","        ctg_name = sorted([os.path.join(\"database/signals\",d) for d in os.listdir(\"database/signals\") if d.endswith(\".csv\")])\n","        #print(ctg_name)\n","        ctg = [pd.read_csv(c, header=0) for c in ctg_name]\n","\n","        #Name of file exclude path\n","        ctg_file_name = []\n","        for name in ctg_name:\n","            file_name = name.replace(\"database/signals/\", \"\")\n","            file_name = file_name.replace(\".csv\", \"\")\n","            file_name = int(file_name)\n","            ctg_file_name.append(file_name)\n","        #print(ctg_file_name)\n","\n","        #Insert Name Column\n","        for i in range(len(ctg)):\n","            name = ctg_file_name[i]\n","            ctg[i].insert(0,\"Name\",name)\n","\n","        df = []\n","        min_df = 0\n","        #Trim\n","        for i in range(len(ctg)):\n","            for j in range(len(ctg[i])):\n","                if((ctg[i].at[j,'FHR']!=0) or (ctg[i].at[j,'UC']!=0)):\n","                    result = j\n","            if min_df==0 or min_df > result:\n","                min_df = result\n","        #print(\"Minimum Signal Length: \", min_df+1)\n","        for i in range(len(ctg)):\n","            #Df without merge\n","            #df.append(ctg[i].iloc[:min_df+1])\n","            #Merge 2 ann and ctg\n","            df.append(pd.merge(ctg[i].iloc[:min_df+1], ann, on=\"Name\"))\n","        return df, ann, min_df+1\n","\n","    def Normalization(self,feature,n):\n","        feature = feature.reshape((len(feature), 1))\n","        scaler = MinMaxScaler(feature_range=(0, 1))\n","\n","        scaler_fit = scaler.fit(feature)\n","        #print(\"Frame: \", n)\n","        #print('Min: %f, Max: %f' % (scaler.data_min_, scaler.data_max_))\n","        return scaler.transform(feature)\n","\n","    def Normalize_X(self,X, name):\n","        #X = X.replace(0, 1)\n","        n = name\n","        for i in range(1,3,1):\n","            if i ==1 :\n","                j= 1;\n","            else:\n","                j = 0.01\n","\n","            X.iloc[:,i] = X.iloc[:,i].replace(0, j)\n","            feature = X.iloc[:,i].values\n","            X.iloc[:,i] = self.Normalization(feature,n)\n","        \n","        return X\n","\n","    def Normalize_y(self,y, name):\n","        n = name\n","        #feature = y.iloc[:,:].values\n","        #y.iloc[:,:] = self.Normalization(feature,n)\n","        feature = y.values\n","        y = self.Normalization(feature,n)\n","        return y\n","\n","    def iterate_train(self,batch_size=16):\n","        total_seqs = self.X_train.shape[1]\n","        permutation = np.random.permutation(total_seqs)\n","        total_batches = total_seqs // batch_size\n","\n","        for i in range(total_batches):\n","            start = i*batch_size\n","            end = start + batch_size\n","            batch_x = self.X_train[:,permutation[start:end]]\n","            batch_y = self.y_train[:,permutation[start:end]]\n","            yield (batch_x,batch_y)\n","\n","    def batching_X(self, X,signal_length,window):\n","        #print(\"X rows: \",X.shape[0])\n","        sn = signal_length\n","        w = window\n","        X = X.to_numpy()\n","        X = X.reshape(X.shape[0]//w,w, X.shape[1])\n","        return X\n","\n","    def batching_y(self, y, signal_length, window):\n","        #print(\"y rows: \", y.shape[0])\n","        sn = signal_length\n","        w = window\n","        y = y.to_numpy()\n","        y = y.reshape(y.shape[0]//w,w, 1)\n","        return y\n","    \n","    def __init__(self,w):\n","        #CTG Set initialize\n","        ctg = []\n","        ctg, ann, signal_length = self.data_frame()\n","        self.window = w\n","        #Split \n","        train, test_valid = train_test_split(ctg, test_size = 0.3, random_state=42)\n","        test, valid = train_test_split(test_valid, test_size = 0.66, random_state=42)\n","        #print(len(train),len(test),len(valid),len(ctg))\n","\n","        #Without Merge\n","        #y = pd.DataFrame(ann.iloc[:,0:1])\n","        X=[]\n","        y=[]\n","        for i in ctg:\n","            X.append(i.iloc[:,1:4])\n","            y.append(i.iloc[:,4])\n","        #Split train, test and validation\n","        X_train, X_test_valid, y_train, y_test_valid = train_test_split(X, y, test_size=0.3, random_state=42)\n","        X_test, X_valid, y_test, y_valid = train_test_split(X_test_valid, y_test_valid, test_size=0.66, random_state=42)\n","        #Concat\n","        X_train = pd.concat(X_train)\n","        X_test = pd.concat(X_test)\n","        X_valid = pd.concat(X_valid)\n","        y_train = pd.concat(y_train)\n","        y_test = pd.concat(y_test)\n","        y_valid = pd.concat(y_valid)\n","        #Normalize X\n","        self.X_train = self.Normalize_X(X_train,'X_train')\n","        self.X_test = self.Normalize_X(X_test,'X_test')\n","        self.X_valid = self.Normalize_X(X_valid,'X_valid')\n","        #Normalize Y\n","        self.y_train = self.Normalize_y(y_train,'y_train')\n","        self.y_test = self.Normalize_y(y_test,'y_test')\n","        self.y_valid = self.Normalize_y(y_valid,'y_valid')\n","        #Batching X\n","        self.X_train = self.batching_X(X_train,signal_length,self.window)\n","        self.X_test = self.batching_X(X_test,signal_length,self.window)\n","        self.X_valid = self.batching_X(X_valid,signal_length,self.window)\n","        #Batching Y without Merge\n","        #self.y = self.batching_y(y)\n","        #self.y_train = self.batching_y(y_train)\n","        #self.y_test = self.batching_y(y_test)\n","        #self.y_valid = self.batching_y(y_valid)\n","\n","        #Batching Y with Merge\n","        self.y_train = self.batching_y(y_train,signal_length,self.window)\n","        self.y_test = self.batching_y(y_test,signal_length,self.window)\n","        self.y_valid = self.batching_y(y_valid,signal_length,self.window)\n","        "],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"QpF9yRqSTiUE","executionInfo":{"status":"ok","timestamp":1632107126099,"user_tz":-420,"elapsed":4,"user":{"displayName":"Tony Wolf","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00046150741472937393"}}},"source":["class TrainingModel:\n","    #Similar - Person\n","    #Similar loss, acc - MSE, MAPE\n","    #Learning Rate: 0.01-0.02 for LTC, 0.001 for all other models.\n","    def __init__(self,window,model_type,model_size,sparsity_level=0.0,learning_rate = 0.001):\n","        self.model_type = model_type\n","        self.window = window\n","        self.constrain_op = []\n","        self.sparsity_level = sparsity_level\n","        self.X = tf.placeholder(dtype=tf.float32,shape=[None,None,3])\n","        self.target_y = tf.placeholder(dtype=tf.float32,shape=[None,None,None])\n","\n","        self.model_size = model_size\n","        head = self.X\n","        \n","        #Print Shape 1\n","        print(\"Head Shape 1\",head.shape)\n","        if(model_type == \"lstm\"):\n","            #unstacked_signal = tf.unstack(self.X,axis=0)\n","            self.fused_cell = tf.nn.rnn_cell.LSTMCell(model_size)\n","\n","            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n","        elif(model_type.startswith(\"ltc\")):\n","            learning_rate = 0.01 # LTC needs a higher learning rate\n","            self.wm = ltc.LTCCell(model_size)\n","            if(model_type.endswith(\"_rk\")):\n","                self.wm._solver = ltc.ODESolver.RungeKutta\n","            elif(model_type.endswith(\"_ex\")):\n","                self.wm._solver = ltc.ODESolver.Explicit\n","            else:\n","                self.wm._solver = ltc.ODESolver.SemiImplicit\n","\n","            head,_ = tf.nn.dynamic_rnn(self.wm,head,dtype=tf.float32,time_major=True)\n","            self.constrain_op.extend(self.wm.get_param_constrain_op())\n","        elif(model_type == \"node\"):\n","            self.fused_cell = NODE(model_size,cell_clip=-1)\n","            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n","        elif(model_type == \"ctgru\"):\n","            self.fused_cell = CTGRU(model_size,cell_clip=-1)\n","            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n","        elif(model_type == \"ctrnn\"):\n","            self.fused_cell = CTRNN(model_size,cell_clip=-1,global_feedback=True)\n","            head,_ = tf.nn.dynamic_rnn(self.fused_cell,head,dtype=tf.float32,time_major=True)\n","        else:\n","            raise ValueError(\"Unknown model type '{}'\".format(model_type))\n","        target_y = tf.expand_dims(self.target_y,axis=-1)\n","        print(target_y.shape)\n","        \n","        #Print Shape 2\n","        print(\"Head Shape 2\",head.shape)\n","        if(self.sparsity_level > 0):\n","            self.constrain_op.extend(self.get_sparsity_ops())\n","        #Change Logit shape\n","        self.y = tf.layers.Dense(1,activation=None)(head)\n","        print(\"logit shape: \",str(self.y.shape))\n","        self.loss = tf.reduce_mean(tf.square(self.target_y-self.y))\n","        optimizer = tf.train.AdamOptimizer(learning_rate)\n","        self.train_step = optimizer.minimize(self.loss)\n","\n","        model_prediction = tf.argmax(input=self.y, axis=2)\n","        mape = tf.keras.losses.MeanAbsolutePercentageError()\n","        self.accuracy = mape(self.target_y, self.y)\n","        self.sess = tf.InteractiveSession()\n","        self.sess.run(tf.global_variables_initializer())\n","\n","        # self.result_file = os.path.join(\"results\",\"ctg\",\"{}_{}_{:02d}.csv\".format(model_type,model_size,int(100*self.sparsity_level)))\n","        self.result_file = os.path.join(\"results\",\"ctg_specific\",\"{}_{}.csv\".format(model_type,model_size))\n","        if(not os.path.exists(\"results/ctg_specific\")):\n","            os.makedirs(\"results/ctg_specific\")\n","        if(not os.path.isfile(self.result_file)):\n","            with open(self.result_file,\"w\") as f:\n","                f.write(\"window size, best epoch, train loss, train mape, valid loss, valid mape, test loss, test mape\\n\")\n","\n","        self.checkpoint_path = os.path.join(\"tf_sessions\",\"ctg_specific\",\"{}\".format(model_type))\n","        if(not os.path.exists(\"tf_sessions/ctg_specific\")):\n","            os.makedirs(\"tf_sessions/ctg_specific\")\n","            \n","        self.saver = tf.train.Saver()\n","\n","    def get_sparsity_ops(self):\n","        tf_vars = tf.trainable_variables()\n","        op_list = []\n","        for v in tf_vars:\n","            # print(\"Variable {}\".format(str(v)))\n","            if(v.name.startswith(\"rnn\")):\n","                if(len(v.shape)<2):\n","                    # Don't sparsity biases\n","                    continue\n","                if(\"ltc\" in v.name and (not \"W:0\" in v.name)):\n","                    # LTC can be sparsified by only setting w[i,j] to 0\n","                    # both input and recurrent matrix will be sparsified\n","                    continue\n","                op_list.append(self.sparse_var(v,self.sparsity_level))\n","                \n","        return op_list\n","        \n","    def sparse_var(self,v,sparsity_level):\n","        mask = np.random.choice([0, 1], size=v.shape, p=[sparsity_level,1-sparsity_level]).astype(np.float32)\n","        v_assign_op = tf.assign(v,v*mask)\n","        print(\"Var[{}] will be sparsified with {:0.2f} sparsity level\".format(\n","            v.name,sparsity_level\n","        ))\n","        return v_assign_op\n","\n","    def save(self):\n","        self.saver.save(self.sess, self.checkpoint_path)\n","\n","    def restore(self):\n","        self.saver.restore(self.sess, self.checkpoint_path)\n","\n","\n","    def fit(self,ctg_data,epochs,verbose=True,log_period=50):\n","\n","        best_valid_loss = np.PINF\n","        best_valid_stats = (0,0,0,0,0,0,0)\n","        self.save()\n","        print(\"Entering training loop\")\n","        #print(\"self.X: \",self.X.shape)\n","        #print(\"ctg_data.X_test: \",ctg_data.X_test.shape)\n","        #print(\"self.target_y\",self.target_y.shape)\n","        #print(\"ctg_data.y_test\",ctg_data.y_test.shape)\n","        #print(\"self.accuracy\",self.accuracy)\n","        #print(\"self.loss\",self.loss)\n","        for e in range(epochs):\n","            if(verbose and e%log_period == 0):\n","                test_acc,test_loss = self.sess.run([self.accuracy,self.loss],{self.X:ctg_data.X_test,self.target_y: ctg_data.y_test})\n","                valid_acc,valid_loss = self.sess.run([self.accuracy,self.loss],{self.X:ctg_data.X_valid,self.target_y: ctg_data.y_valid})\n","                # MSE metric -> less is better\n","                if((valid_loss < best_valid_loss and e > 0) or e==1):\n","                    best_valid_loss = valid_loss\n","                    best_valid_stats = (\n","                        e,\n","                        np.mean(losses),np.mean(accs),\n","                        valid_loss,valid_acc,\n","                        test_loss,test_acc\n","                    )\n","                    self.save()\n","\n","            #Training\n","            print(\"Epoch: \",e)\n","            losses = []\n","            accs = []\n","            for batch_x,batch_y in ctg_data.iterate_train(batch_size=32):\n","                acc,loss,_ = self.sess.run([self.accuracy,self.loss,self.train_step],{self.X:batch_x,self.target_y: batch_y})\n","                if(len(self.constrain_op) > 0):\n","                    self.sess.run(self.constrain_op)\n","\n","                losses.append(loss)\n","                accs.append(acc)\n","                #print(\"loss: \" + str(loss))\n","                #print(\"acc: \" + str(acc))\n","\n","            if(verbose and e%log_period == 0):\n","                print(\"Epochs {:03d}, train loss: {:0.2f}, train mape: {:0.2f}, valid loss: {:0.2f}, valid mape: {:0.2f}, test loss: {:0.2f}, test mape: {:0.2f}\".format(\n","                    e,\n","                    np.mean(losses),np.mean(accs),\n","                    valid_loss,valid_acc,\n","                    test_loss,test_acc\n","                ))\n","            if(e > 0 and (not np.isfinite(np.mean(losses)))):\n","                break\n","        self.restore()\n","        best_epoch,train_loss,train_acc,valid_loss,valid_acc,test_loss,test_acc = best_valid_stats\n","        print(\"Best epoch {:03d}, train loss: {:0.2f}, train mape: {:0.2f}, valid loss: {:0.2f}, valid mape: {:0.2f}, test loss: {:0.2f}, test mape: {:0.2f}\".format(\n","            best_epoch,\n","            train_loss,train_acc,\n","            valid_loss,valid_acc,\n","            test_loss,test_acc\n","        ))\n","        with open(self.result_file,\"a\") as f:\n","            f.write(\"{:03d}, {:03d}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}, {:0.2f}\\n\".format(\n","            self.window,\n","            best_epoch,\n","            train_loss,train_acc,\n","            valid_loss,valid_acc,\n","            test_loss,test_acc\n","        ))"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Sl2iGSpnyaqW","outputId":"94234d48-05f6-4189-dcf8-73ee67811f1e"},"source":["for i in range(100,500,100):\n","    ctg_data = CtgData(i)\n","    print(\"Window: \", i)\n","    tf.reset_default_graph()\n","    model = TrainingModel(window=ctg_data.window, model_type = \"ctrnn\", model_size=32, sparsity_level=0.0)\n","\n","    model.fit(ctg_data ,epochs=200,log_period=1)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Window:  100\n","Head Shape 1 (?, ?, 3)\n","WARNING:tensorflow:From <ipython-input-4-b2801cbdecea>:43: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","(?, ?, ?, 1)\n","Head Shape 2 (?, ?, 32)\n","logit shape:  (?, ?, 1)\n","Entering training loop\n","Epoch:  0\n","Epochs 000, train loss: 27.07, train mape: 71.91, valid loss: 27.28, valid mape: 72.32, test loss: 27.35, test mape: 72.35\n","Epoch:  1\n","Epochs 001, train loss: 26.04, train mape: 70.52, valid loss: 26.24, valid mape: 70.93, test loss: 26.31, test mape: 70.96\n","Epoch:  2\n","Epochs 002, train loss: 24.99, train mape: 69.09, valid loss: 25.22, valid mape: 69.53, test loss: 25.29, test mape: 69.56\n","Epoch:  3\n","Epochs 003, train loss: 22.49, train mape: 65.53, valid loss: 23.03, valid mape: 66.44, test loss: 23.08, test mape: 66.46\n","Epoch:  4\n","Epochs 004, train loss: 21.32, train mape: 63.81, valid loss: 21.55, valid mape: 64.27, test loss: 21.61, test mape: 64.30\n","Epoch:  5\n","Epochs 005, train loss: 18.48, train mape: 59.39, valid loss: 18.72, valid mape: 59.89, test loss: 18.77, test mape: 59.93\n","Epoch:  6\n","Epochs 006, train loss: 17.63, train mape: 58.02, valid loss: 17.78, valid mape: 58.37, test loss: 17.83, test mape: 58.41\n","Epoch:  7\n","Epochs 007, train loss: 16.87, train mape: 56.76, valid loss: 17.01, valid mape: 57.10, test loss: 17.07, test mape: 57.14\n","Epoch:  8\n","Epochs 008, train loss: 16.13, train mape: 55.50, valid loss: 16.27, valid mape: 55.84, test loss: 16.32, test mape: 55.88\n","Epoch:  9\n","Epochs 009, train loss: 15.41, train mape: 54.24, valid loss: 15.54, valid mape: 54.57, test loss: 15.60, test mape: 54.62\n","Epoch:  10\n","Epochs 010, train loss: 14.70, train mape: 52.98, valid loss: 14.84, valid mape: 53.31, test loss: 14.89, test mape: 53.36\n","Epoch:  11\n","Epochs 011, train loss: 14.02, train mape: 51.72, valid loss: 14.14, valid mape: 52.05, test loss: 14.19, test mape: 52.10\n","Epoch:  12\n","Epochs 012, train loss: 13.34, train mape: 50.45, valid loss: 13.47, valid mape: 50.79, test loss: 13.51, test mape: 50.84\n","Epoch:  13\n","Epochs 013, train loss: 12.62, train mape: 49.07, valid loss: 12.79, valid mape: 49.48, test loss: 12.83, test mape: 49.53\n","Epoch:  14\n","Epochs 014, train loss: 10.05, train mape: 43.70, valid loss: 11.75, valid mape: 47.40, test loss: 11.80, test mape: 47.46\n","Epoch:  15\n","Epochs 015, train loss: 8.73, train mape: 40.81, valid loss: 8.81, valid mape: 41.08, test loss: 8.85, test mape: 41.13\n","Epoch:  16\n","Epochs 016, train loss: 8.26, train mape: 39.68, valid loss: 8.34, valid mape: 39.95, test loss: 8.38, test mape: 40.01\n","Epoch:  17\n","Epochs 017, train loss: 7.77, train mape: 38.49, valid loss: 7.87, valid mape: 38.80, test loss: 7.91, test mape: 38.86\n","Epoch:  18\n","Epochs 018, train loss: 6.25, train mape: 34.29, valid loss: 7.31, valid mape: 37.36, test loss: 7.34, test mape: 37.41\n","Epoch:  19\n","Epochs 019, train loss: 4.26, train mape: 28.49, valid loss: 4.30, valid mape: 28.67, test loss: 4.33, test mape: 28.74\n","Epoch:  20\n","Epochs 020, train loss: 4.00, train mape: 27.59, valid loss: 4.03, valid mape: 27.75, test loss: 4.06, test mape: 27.82\n","Epoch:  21\n","Epochs 021, train loss: 3.75, train mape: 26.72, valid loss: 3.78, valid mape: 26.87, test loss: 3.81, test mape: 26.94\n","Epoch:  22\n","Epochs 022, train loss: 3.52, train mape: 25.89, valid loss: 3.55, valid mape: 26.03, test loss: 3.57, test mape: 26.10\n","Epoch:  23\n","Epochs 023, train loss: 3.31, train mape: 25.08, valid loss: 3.33, valid mape: 25.21, test loss: 3.35, test mape: 25.28\n","Epoch:  24\n","Epochs 024, train loss: 3.10, train mape: 24.30, valid loss: 3.13, valid mape: 24.42, test loss: 3.15, test mape: 24.49\n","Epoch:  25\n","Epochs 025, train loss: 2.91, train mape: 23.53, valid loss: 2.93, valid mape: 23.64, test loss: 2.95, test mape: 23.72\n","Epoch:  26\n","Epochs 026, train loss: 2.73, train mape: 22.78, valid loss: 2.75, valid mape: 22.89, test loss: 2.77, test mape: 22.96\n","Epoch:  27\n","Epochs 027, train loss: 2.56, train mape: 22.05, valid loss: 2.58, valid mape: 22.15, test loss: 2.60, test mape: 22.23\n","Epoch:  28\n","Epochs 028, train loss: 2.40, train mape: 21.34, valid loss: 2.41, valid mape: 21.43, test loss: 2.43, test mape: 21.51\n","Epoch:  29\n","Epochs 029, train loss: 2.24, train mape: 20.64, valid loss: 2.26, valid mape: 20.72, test loss: 2.28, test mape: 20.80\n","Epoch:  30\n","Epochs 030, train loss: 2.10, train mape: 19.95, valid loss: 2.11, valid mape: 20.03, test loss: 2.13, test mape: 20.11\n","Epoch:  31\n","Epochs 031, train loss: 1.96, train mape: 19.27, valid loss: 1.97, valid mape: 19.35, test loss: 1.99, test mape: 19.43\n","Epoch:  32\n","Epochs 032, train loss: 1.83, train mape: 18.61, valid loss: 1.84, valid mape: 18.68, test loss: 1.85, test mape: 18.76\n","Epoch:  33\n","Epochs 033, train loss: 1.70, train mape: 17.96, valid loss: 1.71, valid mape: 18.02, test loss: 1.73, test mape: 18.10\n","Epoch:  34\n","Epochs 034, train loss: 1.58, train mape: 17.31, valid loss: 1.59, valid mape: 17.38, test loss: 1.61, test mape: 17.46\n","Epoch:  35\n","Epochs 035, train loss: 1.47, train mape: 16.67, valid loss: 1.48, valid mape: 16.73, test loss: 1.49, test mape: 16.82\n","Epoch:  36\n","Epochs 036, train loss: 1.35, train mape: 15.96, valid loss: 1.37, valid mape: 16.07, test loss: 1.38, test mape: 16.15\n","Epoch:  37\n","Epochs 037, train loss: 1.03, train mape: 13.88, valid loss: 1.17, valid mape: 14.83, test loss: 1.18, test mape: 14.92\n","Epoch:  38\n","Epochs 038, train loss: 0.88, train mape: 12.87, valid loss: 0.88, valid mape: 12.89, test loss: 0.90, test mape: 12.97\n","Epoch:  39\n","Epochs 039, train loss: 0.81, train mape: 12.32, valid loss: 0.81, valid mape: 12.34, test loss: 0.83, test mape: 12.43\n","Epoch:  40\n","Epochs 040, train loss: 0.74, train mape: 11.75, valid loss: 0.75, valid mape: 11.79, test loss: 0.76, test mape: 11.88\n","Epoch:  41\n","Epochs 041, train loss: 0.66, train mape: 10.94, valid loss: 0.68, valid mape: 11.16, test loss: 0.69, test mape: 11.25\n","Epoch:  42\n","Epochs 042, train loss: 0.11, train mape: 4.02, valid loss: 0.16, valid mape: 5.04, test loss: 0.17, test mape: 5.12\n","Epoch:  43\n","Epochs 043, train loss: 0.06, train mape: 3.19, valid loss: 0.06, valid mape: 3.14, test loss: 0.07, test mape: 3.20\n","Epoch:  44\n","Epochs 044, train loss: 0.05, train mape: 2.92, valid loss: 0.05, valid mape: 2.88, test loss: 0.06, test mape: 2.92\n","Epoch:  45\n","Epochs 045, train loss: 0.05, train mape: 2.71, valid loss: 0.05, valid mape: 2.66, test loss: 0.05, test mape: 2.69\n","Epoch:  46\n","Epochs 046, train loss: 0.04, train mape: 2.54, valid loss: 0.04, valid mape: 2.48, test loss: 0.04, test mape: 2.51\n","Epoch:  47\n","Epochs 047, train loss: 0.04, train mape: 2.39, valid loss: 0.04, valid mape: 2.35, test loss: 0.04, test mape: 2.37\n","Epoch:  48\n","Epochs 048, train loss: 0.03, train mape: 2.28, valid loss: 0.03, valid mape: 2.23, test loss: 0.04, test mape: 2.25\n","Epoch:  49\n","Epochs 049, train loss: 0.03, train mape: 2.17, valid loss: 0.03, valid mape: 2.13, test loss: 0.03, test mape: 2.14\n","Epoch:  50\n","Epochs 050, train loss: 0.03, train mape: 2.08, valid loss: 0.03, valid mape: 2.04, test loss: 0.03, test mape: 2.05\n","Epoch:  51\n","Epochs 051, train loss: 0.03, train mape: 2.00, valid loss: 0.03, valid mape: 1.96, test loss: 0.03, test mape: 1.98\n","Epoch:  52\n","Epochs 052, train loss: 0.03, train mape: 1.93, valid loss: 0.03, valid mape: 1.89, test loss: 0.03, test mape: 1.91\n","Epoch:  53\n","Epochs 053, train loss: 0.02, train mape: 1.87, valid loss: 0.02, valid mape: 1.82, test loss: 0.03, test mape: 1.85\n","Epoch:  54\n","Epochs 054, train loss: 0.02, train mape: 1.81, valid loss: 0.02, valid mape: 1.76, test loss: 0.02, test mape: 1.79\n","Epoch:  55\n","Epochs 055, train loss: 0.02, train mape: 1.76, valid loss: 0.02, valid mape: 1.71, test loss: 0.02, test mape: 1.74\n","Epoch:  56\n","Epochs 056, train loss: 0.02, train mape: 1.71, valid loss: 0.02, valid mape: 1.66, test loss: 0.02, test mape: 1.69\n","Epoch:  57\n","Epochs 057, train loss: 0.02, train mape: 1.67, valid loss: 0.02, valid mape: 1.62, test loss: 0.02, test mape: 1.65\n","Epoch:  58\n","Epochs 058, train loss: 0.02, train mape: 1.63, valid loss: 0.02, valid mape: 1.58, test loss: 0.02, test mape: 1.62\n","Epoch:  59\n","Epochs 059, train loss: 0.02, train mape: 1.59, valid loss: 0.02, valid mape: 1.55, test loss: 0.02, test mape: 1.58\n","Epoch:  60\n","Epochs 060, train loss: 0.02, train mape: 1.56, valid loss: 0.02, valid mape: 1.52, test loss: 0.02, test mape: 1.55\n","Epoch:  61\n","Epochs 061, train loss: 0.02, train mape: 1.53, valid loss: 0.02, valid mape: 1.49, test loss: 0.02, test mape: 1.52\n","Epoch:  62\n","Epochs 062, train loss: 0.02, train mape: 1.50, valid loss: 0.02, valid mape: 1.46, test loss: 0.02, test mape: 1.49\n","Epoch:  63\n","Epochs 063, train loss: 0.02, train mape: 1.47, valid loss: 0.02, valid mape: 1.43, test loss: 0.02, test mape: 1.47\n","Epoch:  64\n","Epochs 064, train loss: 0.02, train mape: 1.45, valid loss: 0.02, valid mape: 1.41, test loss: 0.02, test mape: 1.45\n","Epoch:  65\n","Epochs 065, train loss: 0.02, train mape: 1.43, valid loss: 0.02, valid mape: 1.39, test loss: 0.02, test mape: 1.42\n","Epoch:  66\n","Epochs 066, train loss: 0.02, train mape: 1.41, valid loss: 0.02, valid mape: 1.37, test loss: 0.02, test mape: 1.40\n","Epoch:  67\n","Epochs 067, train loss: 0.01, train mape: 1.39, valid loss: 0.02, valid mape: 1.35, test loss: 0.02, test mape: 1.39\n","Epoch:  68\n","Epochs 068, train loss: 0.01, train mape: 1.37, valid loss: 0.02, valid mape: 1.34, test loss: 0.02, test mape: 1.37\n","Epoch:  69\n","Epochs 069, train loss: 0.01, train mape: 1.35, valid loss: 0.02, valid mape: 1.32, test loss: 0.02, test mape: 1.36\n","Epoch:  70\n","Epochs 070, train loss: 0.01, train mape: 1.34, valid loss: 0.02, valid mape: 1.31, test loss: 0.02, test mape: 1.34\n","Epoch:  71\n","Epochs 071, train loss: 0.01, train mape: 1.32, valid loss: 0.02, valid mape: 1.30, test loss: 0.02, test mape: 1.33\n","Epoch:  72\n","Epochs 072, train loss: 0.01, train mape: 1.31, valid loss: 0.01, valid mape: 1.28, test loss: 0.02, test mape: 1.32\n","Epoch:  73\n","Epochs 073, train loss: 0.01, train mape: 1.30, valid loss: 0.01, valid mape: 1.27, test loss: 0.02, test mape: 1.30\n","Epoch:  74\n","Epochs 074, train loss: 0.01, train mape: 1.29, valid loss: 0.01, valid mape: 1.27, test loss: 0.02, test mape: 1.29\n","Epoch:  75\n","Epochs 075, train loss: 0.01, train mape: 1.28, valid loss: 0.01, valid mape: 1.26, test loss: 0.02, test mape: 1.28\n","Epoch:  76\n","Epochs 076, train loss: 0.01, train mape: 1.27, valid loss: 0.01, valid mape: 1.25, test loss: 0.02, test mape: 1.27\n","Epoch:  77\n","Epochs 077, train loss: 0.01, train mape: 1.26, valid loss: 0.01, valid mape: 1.24, test loss: 0.01, test mape: 1.26\n","Epoch:  78\n","Epochs 078, train loss: 0.01, train mape: 1.25, valid loss: 0.01, valid mape: 1.23, test loss: 0.01, test mape: 1.25\n","Epoch:  79\n","Epochs 079, train loss: 0.01, train mape: 1.24, valid loss: 0.01, valid mape: 1.23, test loss: 0.01, test mape: 1.24\n","Epoch:  80\n","Epochs 080, train loss: 0.01, train mape: 1.24, valid loss: 0.01, valid mape: 1.22, test loss: 0.01, test mape: 1.24\n","Epoch:  81\n","Epochs 081, train loss: 0.01, train mape: 1.23, valid loss: 0.01, valid mape: 1.22, test loss: 0.01, test mape: 1.23\n","Epoch:  82\n","Epochs 082, train loss: 0.01, train mape: 1.22, valid loss: 0.01, valid mape: 1.21, test loss: 0.01, test mape: 1.22\n","Epoch:  83\n","Epochs 083, train loss: 0.01, train mape: 1.22, valid loss: 0.01, valid mape: 1.21, test loss: 0.01, test mape: 1.21\n","Epoch:  84\n","Epochs 084, train loss: 0.01, train mape: 1.21, valid loss: 0.01, valid mape: 1.20, test loss: 0.01, test mape: 1.21\n","Epoch:  85\n","Epochs 085, train loss: 0.01, train mape: 1.21, valid loss: 0.01, valid mape: 1.20, test loss: 0.01, test mape: 1.20\n","Epoch:  86\n","Epochs 086, train loss: 0.01, train mape: 1.20, valid loss: 0.01, valid mape: 1.20, test loss: 0.01, test mape: 1.20\n","Epoch:  87\n","Epochs 087, train loss: 0.01, train mape: 1.20, valid loss: 0.01, valid mape: 1.19, test loss: 0.01, test mape: 1.20\n","Epoch:  88\n","Epochs 088, train loss: 0.01, train mape: 1.20, valid loss: 0.01, valid mape: 1.19, test loss: 0.01, test mape: 1.19\n","Epoch:  89\n","Epochs 089, train loss: 0.01, train mape: 1.19, valid loss: 0.01, valid mape: 1.19, test loss: 0.01, test mape: 1.19\n","Epoch:  90\n","Epochs 090, train loss: 0.01, train mape: 1.19, valid loss: 0.01, valid mape: 1.19, test loss: 0.01, test mape: 1.19\n","Epoch:  91\n","Epochs 091, train loss: 0.01, train mape: 1.19, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.18\n","Epoch:  92\n","Epochs 092, train loss: 0.01, train mape: 1.18, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.18\n","Epoch:  93\n","Epochs 093, train loss: 0.01, train mape: 1.18, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.18\n","Epoch:  94\n","Epochs 094, train loss: 0.01, train mape: 1.18, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.18\n","Epoch:  95\n","Epochs 095, train loss: 0.01, train mape: 1.17, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.18\n","Epoch:  96\n","Epochs 096, train loss: 0.01, train mape: 1.17, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.18\n","Epoch:  97\n","Epochs 097, train loss: 0.01, train mape: 1.17, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.18\n","Epoch:  98\n","Epochs 098, train loss: 0.01, train mape: 1.17, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.17\n","Epoch:  99\n","Epochs 099, train loss: 0.01, train mape: 1.17, valid loss: 0.01, valid mape: 1.18, test loss: 0.01, test mape: 1.17\n","Epoch:  100\n","Epochs 100, train loss: 0.01, train mape: 1.17, valid loss: 0.01, valid mape: 1.17, test loss: 0.01, test mape: 1.17\n","Epoch:  101\n","Epochs 101, train loss: 0.01, train mape: 1.16, valid loss: 0.01, valid mape: 1.17, test loss: 0.01, test mape: 1.17\n","Epoch:  102\n","Epochs 102, train loss: 0.01, train mape: 1.16, valid loss: 0.01, valid mape: 1.17, test loss: 0.01, test mape: 1.17\n","Epoch:  103\n"]}]},{"cell_type":"code","metadata":{"id":"Z6eW_P4eWgoP"},"source":[""],"execution_count":null,"outputs":[]}]}